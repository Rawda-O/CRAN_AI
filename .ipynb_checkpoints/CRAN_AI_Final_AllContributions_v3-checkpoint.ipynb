{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18fc0b80",
   "metadata": {},
   "source": [
    "# CRAN‑AI — Final Notebook (All Contributions)\n",
    "\n",
    "This notebook implements and evaluates the full set of contributions in the latest `cran_ai` codebase:\n",
    "\n",
    "1. **Multi‑objective unsupervised training** (Rate + QoS + EE + Outage proxy)  \n",
    "2. **Risk‑aware training** (CVaR / chance‑style penalty)  \n",
    "3. **Hard safety layer** (deterministic QoS satisfaction)  \n",
    "4. **DF policy** and **CF policy** (AI‑native normalized action space)  \n",
    "5. **Hybrid DF/CF selection** (rule‑based)  \n",
    "6. **Brute‑force baselines** (DF and CF)  \n",
    "7. **Generalized policy** (system parameters appended to input)  \n",
    "8. **Robust CSI pairing** (imperfect input, perfect physics loss)  \n",
    "9. **Selector network** (supervised DF/CF classifier + threshold tuning)  \n",
    "10. **Deployment indicators** (params, size, latency benchmark + export)\n",
    "\n",
    "Run all cells top‑to‑bottom. Only edit the **Parameters** cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b2c9e",
   "metadata": {},
   "source": [
    "## Notation and Objectives\n",
    "\n",
    "### Channel gains\n",
    "Let the link gains be stacked as\n",
    "$$\n",
    "\\mathbf{g}=\\left[g_{SS}, g_{SR}, g_{RS}, g_{SP}, g_{RP}, g_{PP}, g_{PR}, g_{PS}\\right].\n",
    "$$\n",
    "\n",
    "### AI‑native action space (normalized budgets)\n",
    "$$\n",
    "P_S=\\gamma_S\\,P_{S,\\max},\\quad P_R=\\gamma_R\\,P_{R,\\max},\\quad \\gamma_S,\\gamma_R\\in[0,1],\n",
    "$$\n",
    "and DF includes \\(\\alpha\\in[0,1]\\).\n",
    "\n",
    "### Primary QoS protection\n",
    "Define an interference‑like metric \\(Q(\\cdot)\\) and a QoS budget \\(A(\\cdot)\\). The constraint is\n",
    "$$\n",
    "Q \\le A.\n",
    "$$\n",
    "A **safety layer** rescales \\((P_S,P_R)\\) to guarantee feasibility.\n",
    "\n",
    "### Energy efficiency\n",
    "$$\n",
    "EE=\\frac{R_S}{P_S+P_R+P_c}.\n",
    "$$\n",
    "\n",
    "### Outage proxy\n",
    "For target \\(R_{S,\\text{th}}\\),\n",
    "$$\n",
    "\\mathrm{OutageProxy}=\\sigma\\!\\left(k\\,(R_{S,\\text{th}}-R_S)\\right).\n",
    "$$\n",
    "\n",
    "### Multi‑objective loss (minimized)\n",
    "$$\n",
    "\\mathcal{L}\n",
    "= -w_r\\,\\mathbb{E}[R_S]\n",
    "+ w_q\\,\\mathbb{E}\\big[(Q-A)^+\\big]\n",
    "+ w_e\\,\\mathbb{E}\\big[-EE\\big]\n",
    "+ w_o\\,\\mathbb{E}[\\mathrm{OutageProxy}]\n",
    "+ \\mathcal{R}_{\\text{risk}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65877381",
   "metadata": {},
   "source": [
    "## Environment and Imports\n",
    "\n",
    "The project must be run from the repository root (the folder that contains `cran/`, `configs/`, `experiments/`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"cran\").exists():\n",
    "    raise RuntimeError(f\"Run from project root. Current: {PROJECT_ROOT}\")\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch :\", torch.__version__)\n",
    "print(\"CUDA  :\", torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
    "\n",
    "# Project imports\n",
    "from cran.utils.config_loader import merge_configs, save_config_snapshot\n",
    "from cran.utils.seed import SeedConfig, set_global_seed, get_torch_device\n",
    "\n",
    "from cran.reporting.artifact_registry import ArtifactPaths\n",
    "\n",
    "from cran.policies.df_policy import DFPolicy\n",
    "from cran.policies.cf_policy import CFPolicy\n",
    "from cran.policies.generalized_policy import GeneralizedDFPolicy, GeneralizedCFPolicy\n",
    "\n",
    "from cran.data.feature_builder import FeatureConfig, build_features\n",
    "from cran.data.dataset import ChannelBatchConfig, ChannelBatchGenerator\n",
    "from cran.physics.csi import ImperfectCSIConfig, make_imperfect_pair\n",
    "\n",
    "from cran.physics.channels import generate_gains, GeometryConfig, FadingConfig, LINKS\n",
    "from cran.physics.rates_df import DFParams, secondary_rate_df, qos_A as qos_A_df, qos_Q_df\n",
    "from cran.physics.rates_cf import CFParams, secondary_rate_cf, qos_A as qos_A_cf, qos_Q_cf\n",
    "from cran.physics.constraints import qos_violation, safety_scale_joint_df, safety_scale_joint_cf\n",
    "from cran.physics.outage import outage_hard\n",
    "from cran.physics.energy import ee_secondary\n",
    "\n",
    "from cran.learning.losses import MultiObjectiveWeights, RiskConfig\n",
    "from cran.learning.steps import StepConfig, df_step_fn, cf_step_fn\n",
    "from cran.learning.schedulers import build_scheduler\n",
    "\n",
    "from cran.selection.rule_based import select_best_scheme\n",
    "from cran.selection.selector_net import SelectorNet\n",
    "from cran.selection.threshold_tuning import tune_threshold\n",
    "\n",
    "from cran.baselines.bruteforce_df import solve_df_bruteforce, DFGrid\n",
    "from cran.baselines.bruteforce_cf import solve_cf_bruteforce, CFGrid\n",
    "\n",
    "from cran.deploy.model_summary import count_parameters, model_size_mb\n",
    "from cran.deploy.benchmark_latency import benchmark_model, BenchmarkConfig\n",
    "from cran.deploy.export_torchscript import export_torchscript\n",
    "from cran.deploy.export_onnx import export_onnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c68e70",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "### Effective sample size (on‑the‑fly data)\n",
    "Training batches are generated online. The effective number of training samples is:\n",
    "$$\n",
    "N_{\\text{eff}} = \\text{epochs} \\times \\text{steps\\_per\\_epoch} \\times \\text{batch}.\n",
    "$$\n",
    "\n",
    "Edit only this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (edit only this cell)\n",
    "\n",
    "RUN_TAG = \"final_all\"\n",
    "\n",
    "DEVICE_PREFER = \"cuda\"      # \"cuda\" or \"cpu\"\n",
    "AMP = True                  # recommended on small VRAM GPUs\n",
    "\n",
    "SEED = 123\n",
    "DETERMINISTIC = True\n",
    "\n",
    "# ====== Training sizes (effective samples) ======\n",
    "EPOCHS_BASE = 70\n",
    "BATCH_BASE = 256\n",
    "TRAIN_SAMPLES_BASE = 14_336_000\n",
    "STEPS_BASE = int(np.ceil(TRAIN_SAMPLES_BASE / (EPOCHS_BASE * BATCH_BASE)))\n",
    "\n",
    "EPOCHS_GENROB = 60\n",
    "BATCH_GENROB = 256\n",
    "TRAIN_SAMPLES_GENROB = 10_000_000\n",
    "STEPS_GENROB = int(np.ceil(TRAIN_SAMPLES_GENROB / (EPOCHS_GENROB * BATCH_GENROB)))\n",
    "\n",
    "EPOCHS_SELECTOR = 15\n",
    "BATCH_SELECTOR = 1024\n",
    "\n",
    "# ====== Evaluation sizes ======\n",
    "SNR_DB = [0, 5, 10, 15, 20, 25, 30]\n",
    "EVAL_BATCH = 512\n",
    "EVAL_SAMPLES_PER_SNR = 200_000\n",
    "EVAL_STEPS = int(np.ceil(EVAL_SAMPLES_PER_SNR / EVAL_BATCH))\n",
    "\n",
    "# Robustness evaluation: CSI noise sigma sweep (applied only to gains, not system params)\n",
    "ROBUST_SIGMAS = [0.0, 0.05, 0.10, 0.15, 0.20]\n",
    "\n",
    "# Generalization evaluation grid (uses generalized model)\n",
    "TAU_GRID = np.linspace(0.1, 0.9, 9).tolist()\n",
    "PS_MAX_GRID = [2.0, 5.0, 10.0]\n",
    "PR_MAX_GRID = [2.0, 5.0, 10.0]\n",
    "GEN_EVAL_SNR_DB = 20\n",
    "\n",
    "# Brute-force grids (accuracy vs time)\n",
    "DF_GRID = dict(ps_steps=21, pr_steps=21, alpha_steps=11)\n",
    "CF_GRID = dict(ps_steps=41, pr_steps=41)\n",
    "\n",
    "# Selector features\n",
    "SELECTOR_INCLUDE_POLICY_OUTPUTS = True  # gains + predicted gammas (stronger selector)\n",
    "\n",
    "# ====== Stages to run ======\n",
    "RUN_BASE_TRAIN = True          # DF + CF (standard, 8-dim input)\n",
    "RUN_GENROB_TRAIN = True        # Generalized + Robust DF + CF (11-dim input, robust pairing)\n",
    "RUN_SELECTOR_TRAIN = True      # Supervised selector net + threshold tuning\n",
    "RUN_EVALUATION = True          # plots + JSON/CSV\n",
    "RUN_DEPLOYMENT = True          # params/size/latency + export\n",
    "\n",
    "# Config files\n",
    "CFG_BASE = \"configs/base.yaml\"\n",
    "CFG_CHANNEL = \"configs/channel.yaml\"\n",
    "CFG_DF = \"configs/df.yaml\"\n",
    "CFG_CF = \"configs/cf.yaml\"\n",
    "CFG_GENERALIZED = \"configs/generalized.yaml\"\n",
    "CFG_ROBUST = \"configs/robust.yaml\"\n",
    "CFG_MULTI = \"configs/multi_objective.yaml\"\n",
    "CFG_RISK = \"configs/risk.yaml\"\n",
    "CFG_SAFETY = \"configs/safety.yaml\"\n",
    "CFG_SELECTOR = \"configs/selector.yaml\"\n",
    "CFG_OPTIMIZER = \"configs/optimizer.yaml\"  # auto-created if missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f6547",
   "metadata": {},
   "source": [
    "## Run Plan\n",
    "\n",
    "The tables below are printed before training and before evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run plan tables\n",
    "\n",
    "def _plan_df(rows):\n",
    "    return pd.DataFrame(rows, columns=[\"key\", \"value\"])\n",
    "\n",
    "effective_base = int(EPOCHS_BASE * STEPS_BASE * BATCH_BASE)\n",
    "effective_genrob = int(EPOCHS_GENROB * STEPS_GENROB * BATCH_GENROB)\n",
    "effective_eval = int(EVAL_STEPS * EVAL_BATCH)\n",
    "\n",
    "plan_main = _plan_df([\n",
    "    (\"RUN_TAG\", RUN_TAG),\n",
    "    (\"DEVICE_PREFER\", DEVICE_PREFER),\n",
    "    (\"AMP\", AMP),\n",
    "    (\"SEED\", SEED),\n",
    "    (\"EVAL_SNR_DB\", str(SNR_DB)),\n",
    "    (\"EVAL_SAMPLES_PER_SNR\", f\"{effective_eval:,}\"),\n",
    "    (\"DF_GRID\", str(DF_GRID)),\n",
    "    (\"CF_GRID\", str(CF_GRID)),\n",
    "])\n",
    "\n",
    "plan_base = _plan_df([\n",
    "    (\"RUN_BASE_TRAIN\", RUN_BASE_TRAIN),\n",
    "    (\"EPOCHS_BASE\", EPOCHS_BASE),\n",
    "    (\"BATCH_BASE\", BATCH_BASE),\n",
    "    (\"STEPS_BASE\", STEPS_BASE),\n",
    "    (\"EFFECTIVE_SAMPLES_BASE\", f\"{effective_base:,}\"),\n",
    "])\n",
    "\n",
    "plan_genrob = _plan_df([\n",
    "    (\"RUN_GENROB_TRAIN\", RUN_GENROB_TRAIN),\n",
    "    (\"EPOCHS_GENROB\", EPOCHS_GENROB),\n",
    "    (\"BATCH_GENROB\", BATCH_GENROB),\n",
    "    (\"STEPS_GENROB\", STEPS_GENROB),\n",
    "    (\"EFFECTIVE_SAMPLES_GENROB\", f\"{effective_genrob:,}\"),\n",
    "    (\"ROBUST_SIGMAS\", str(ROBUST_SIGMAS)),\n",
    "    (\"TAU_GRID\", str(TAU_GRID)),\n",
    "    (\"PS_MAX_GRID\", str(PS_MAX_GRID)),\n",
    "    (\"PR_MAX_GRID\", str(PR_MAX_GRID)),\n",
    "])\n",
    "\n",
    "plan_selector = _plan_df([\n",
    "    (\"RUN_SELECTOR_TRAIN\", RUN_SELECTOR_TRAIN),\n",
    "    (\"EPOCHS_SELECTOR\", EPOCHS_SELECTOR),\n",
    "    (\"BATCH_SELECTOR\", BATCH_SELECTOR),\n",
    "    (\"SELECTOR_INCLUDE_POLICY_OUTPUTS\", SELECTOR_INCLUDE_POLICY_OUTPUTS),\n",
    "])\n",
    "\n",
    "display(plan_main)\n",
    "display(plan_base)\n",
    "display(plan_genrob)\n",
    "display(plan_selector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb2348",
   "metadata": {},
   "source": [
    "## Configuration Merge\n",
    "\n",
    "All YAMLs are merged (later overrides earlier). Numeric normalization is applied to avoid YAML scientific-notation parsing pitfalls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge configs + normalization + reproducibility\n",
    "\n",
    "# Auto-create optimizer.yaml if missing\n",
    "opt_path = Path(CFG_OPTIMIZER)\n",
    "if not opt_path.exists():\n",
    "    opt_path.write_text(\n",
    "        \"optimizer:\\n\"\n",
    "        \"  lr: 0.001\\n\"\n",
    "        \"  weight_decay: 0.0001\\n\"\n",
    "        \"  grad_clip_norm: 1.0\\n\"\n",
    "        \"  scheduler: cosine\\n\"\n",
    "        \"\\n\"\n",
    "        \"device:\\n\"\n",
    "        \"  amp: true\\n\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"Created:\", opt_path)\n",
    "\n",
    "cfg_paths = [CFG_BASE, CFG_OPTIMIZER, CFG_CHANNEL, CFG_DF, CFG_CF, CFG_GENERALIZED, CFG_ROBUST, CFG_MULTI, CFG_RISK, CFG_SAFETY, CFG_SELECTOR]\n",
    "cfg_paths = [p for p in cfg_paths if Path(p).exists()]\n",
    "\n",
    "cfg = merge_configs(cfg_paths)\n",
    "\n",
    "# Normalize numeric fields\n",
    "geo = cfg.get(\"channel\", {}).get(\"geometry\", {})\n",
    "if \"min_distance\" in geo:\n",
    "    geo[\"min_distance\"] = float(geo[\"min_distance\"])\n",
    "cfg[\"channel\"][\"geometry\"] = geo\n",
    "\n",
    "cfg.setdefault(\"device\", {})\n",
    "cfg[\"device\"][\"prefer\"] = DEVICE_PREFER\n",
    "cfg[\"device\"][\"amp\"] = bool(AMP)\n",
    "\n",
    "device = get_torch_device(cfg[\"device\"][\"prefer\"])\n",
    "set_global_seed(SeedConfig(\n",
    "    value=int(SEED),\n",
    "    deterministic=bool(DETERMINISTIC),\n",
    "    cudnn_benchmark=bool(cfg.get(\"device\", {}).get(\"cudnn_benchmark\", False))\n",
    "))\n",
    "\n",
    "print(\"Merged configs:\")\n",
    "for p in cfg_paths:\n",
    "    print(\" -\", p)\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cfaec2",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "For each epoch \\(e\\) we report a single dynamic progress line containing:\n",
    "$$\n",
    "(\\mathcal{L}, \\mathbb{E}[R_S], \\mathbb{E}[(Q-A)^+], \\mathbb{E}[\\mathrm{OutageProxy}], \\mathbb{E}[EE], \\mathrm{lr}, \\mathrm{sec})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers (one progress line per epoch)\n",
    "\n",
    "def make_artifacts(stage: str) -> ArtifactPaths:\n",
    "    return ArtifactPaths.make(Path(\"outputs\") / RUN_TAG / stage)\n",
    "\n",
    "def save_csv(df: pd.DataFrame, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def _optimizer_and_sched(model: torch.nn.Module, opt_cfg: dict, max_steps: int):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=float(opt_cfg[\"lr\"]), weight_decay=float(opt_cfg[\"weight_decay\"]))\n",
    "    sched_name = str(opt_cfg.get(\"scheduler\", \"cosine\"))\n",
    "    sched = build_scheduler(opt, sched_name, max_steps=max_steps) if sched_name else None\n",
    "    return opt, sched\n",
    "\n",
    "def _amp_scaler(enabled: bool):\n",
    "    return torch.amp.GradScaler(\"cuda\", enabled=(enabled and device.type == \"cuda\"))\n",
    "\n",
    "def train_policy(\n",
    "    *,\n",
    "    stage: str,\n",
    "    model: torch.nn.Module,\n",
    "    step_fn,\n",
    "    epoch_iterator_fn,\n",
    "    epochs: int,\n",
    "    steps_per_epoch: int,\n",
    "    mixed_precision: bool,\n",
    "    opt_cfg: dict,\n",
    "    grad_clip_norm: float,\n",
    "):\n",
    "    ap = make_artifacts(stage)\n",
    "    save_config_snapshot(cfg, ap.root / \"config_merged.yaml\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer, sched = _optimizer_and_sched(model, opt_cfg, max_steps=int(epochs * steps_per_epoch))\n",
    "    scaler = _amp_scaler(mixed_precision)\n",
    "\n",
    "    rows = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    pbar = tqdm(range(1, epochs + 1), desc=f\"train:{stage}\", dynamic_ncols=True)\n",
    "    for ep in pbar:\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "\n",
    "        sums = {\"loss\": 0.0, \"rs\": 0.0, \"viol\": 0.0, \"out\": 0.0, \"ee\": 0.0}\n",
    "        n = 0\n",
    "\n",
    "        for batch in epoch_iterator_fn(steps_per_epoch):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(mixed_precision and device.type == \"cuda\")):\n",
    "                out = step_fn(model, batch, device)\n",
    "                loss = out[\"loss\"]\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if grad_clip_norm and grad_clip_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip_norm))\n",
    "\n",
    "            # Ensure scheduler steps only if optimizer really stepped (avoids warning)\n",
    "            if mixed_precision and device.type == \"cuda\":\n",
    "                scale_before = float(scaler.get_scale())\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scale_after = float(scaler.get_scale())\n",
    "                did_step = (scale_after >= scale_before)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                did_step = True\n",
    "            if (sched is not None) and did_step:\n",
    "                sched.step()\n",
    "            sums[\"loss\"] += float(out[\"loss\"].detach().cpu().item())\n",
    "            sums[\"rs\"]   += float(out[\"rs_mean\"].detach().cpu().item())\n",
    "            sums[\"viol\"] += float(out[\"qos_violation_mean\"].detach().cpu().item())\n",
    "            sums[\"out\"]  += float(out[\"outage_proxy_mean\"].detach().cpu().item())\n",
    "            sums[\"ee\"]   += float(out[\"ee_mean\"].detach().cpu().item())\n",
    "            n += 1\n",
    "\n",
    "        mean = {k: v / max(n, 1) for k, v in sums.items()}\n",
    "        lr_now = float(optimizer.param_groups[0][\"lr\"])\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        rows.append({\n",
    "            \"epoch\": ep,\n",
    "            \"loss\": mean[\"loss\"],\n",
    "            \"rs_mean\": mean[\"rs\"],\n",
    "            \"qos_violation_mean\": mean[\"viol\"],\n",
    "            \"outage_proxy_mean\": mean[\"out\"],\n",
    "            \"ee_mean\": mean[\"ee\"],\n",
    "            \"lr\": lr_now,\n",
    "            \"sec\": dt,\n",
    "        })\n",
    "\n",
    "        # checkpoints\n",
    "        ckpt_last = ap.checkpoints / \"last.pt\"\n",
    "        ckpt_best = ap.checkpoints / \"best.pt\"\n",
    "        torch.save({\"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict(), \"epoch\": ep}, ckpt_last)\n",
    "        if mean[\"loss\"] < best_loss:\n",
    "            best_loss = mean[\"loss\"]\n",
    "            torch.save({\"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict(), \"epoch\": ep, \"best_loss\": best_loss}, ckpt_best)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{mean['loss']:.4f}\",\n",
    "            \"rs\": f\"{mean['rs']:.4f}\",\n",
    "            \"viol\": f\"{mean['viol']:.2e}\",\n",
    "            \"out\": f\"{mean['out']:.3f}\",\n",
    "            \"ee\": f\"{mean['ee']:.3f}\",\n",
    "            \"lr\": f\"{lr_now:.2e}\",\n",
    "            \"s\": f\"{dt:.1f}\",\n",
    "        })\n",
    "\n",
    "    dfm = pd.DataFrame(rows)\n",
    "    save_csv(dfm, ap.results / f\"metrics_{stage}.csv\")\n",
    "    return ap, dfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975b56a",
   "metadata": {},
   "source": [
    "## Physics and Loss Configs\n",
    "\n",
    "The step functions compute actions from \\(x\\) (possibly imperfect) and evaluate physics using \\(g_{\\text{perfect}}\\) for robust pairing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build physics/loss configs\n",
    "\n",
    "df_cfg = cfg[\"df\"]\n",
    "cf_cfg = cfg[\"cf\"]\n",
    "\n",
    "dfp_nom = DFParams(\n",
    "    tau=float(df_cfg[\"tau\"]),\n",
    "    noise_power=float(df_cfg[\"noise_power\"]),\n",
    "    Pp=float(df_cfg[\"power\"][\"Pp\"]),\n",
    "    Rp0=float(df_cfg[\"primary_qos\"][\"Rp0\"]),\n",
    ")\n",
    "cfp_nom = CFParams(\n",
    "    tau=float(cf_cfg[\"tau\"]),\n",
    "    noise_power=float(cf_cfg[\"noise_power\"]),\n",
    "    Pp=float(cf_cfg[\"power\"][\"Pp\"]),\n",
    "    Rp0=float(cf_cfg[\"primary_qos\"][\"Rp0\"]),\n",
    ")\n",
    "\n",
    "wcfg = cfg[\"multi_objective\"][\"weights\"]\n",
    "w = MultiObjectiveWeights(wr=float(wcfg[\"wr\"]), wq=float(wcfg[\"wq\"]), we=float(wcfg[\"we\"]), wo=float(wcfg[\"wo\"]))\n",
    "\n",
    "rcfg = cfg.get(\"risk\", {})\n",
    "risk = RiskConfig(\n",
    "    enable=bool(rcfg.get(\"enable\", False)),\n",
    "    type=str(rcfg.get(\"type\", \"cvar\")),\n",
    "    cvar_alpha=float(rcfg.get(\"cvar_alpha\", 0.95)),\n",
    "    epsilon=float(rcfg.get(\"epsilon\", 0.01)),\n",
    "    weight=float(rcfg.get(\"weight\", 5.0)),\n",
    ")\n",
    "\n",
    "safety_on = bool(cfg.get(\"safety\", {}).get(\"enable\", True))\n",
    "rs_th = float(cfg[\"multi_objective\"][\"outage\"][\"Rs_threshold\"])\n",
    "out_k = float(cfg[\"multi_objective\"][\"outage\"][\"k\"])\n",
    "\n",
    "opt_cfg = cfg[\"optimizer\"]\n",
    "grad_clip = float(opt_cfg.get(\"grad_clip_norm\", 1.0))\n",
    "\n",
    "print(\"weights:\", asdict(w))\n",
    "print(\"risk   :\", asdict(risk))\n",
    "print(\"safety :\", safety_on, \"Rs_th:\", rs_th, \"k:\", out_k)\n",
    "print(\"opt    :\", opt_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138146e",
   "metadata": {},
   "source": [
    "## Data Generators\n",
    "\n",
    "### Robust pairing\n",
    "$$\n",
    "(g_{\\text{imperfect}}, g_{\\text{perfect}})=\\mathrm{CSI\\_pair}(g,\\sigma)\n",
    "$$\n",
    "Actions use \\(g_{\\text{imperfect}}\\) but loss uses \\(g_{\\text{perfect}}\\).\n",
    "\n",
    "### Generalization\n",
    "The input is extended to:\n",
    "$$\n",
    "x=[\\mathbf{g}, \\tau, P_{S,\\max}, P_{R,\\max}]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch iterator factories\n",
    "\n",
    "ch = cfg[\"channel\"]\n",
    "geo = GeometryConfig(**ch[\"geometry\"])\n",
    "fad = FadingConfig(**ch[\"fading\"])\n",
    "\n",
    "def _gen_base(batch_size: int, robust: bool, sigma: float = 0.0):\n",
    "    bcfg = ChannelBatchConfig(\n",
    "        batch_size=int(batch_size),\n",
    "        channel_model=ch[\"model\"],\n",
    "        mixed_weights=ch.get(\"mixed\", {}).get(\"weights\"),\n",
    "        geometry=ch.get(\"geometry\", {}),\n",
    "        fading=ch.get(\"fading\", {}),\n",
    "    )\n",
    "    icfg = None\n",
    "    if robust:\n",
    "        ncfg = cfg[\"robust\"][\"noise\"]\n",
    "        icfg = ImperfectCSIConfig(\n",
    "            sigma=float(sigma if sigma > 0 else ncfg[\"sigma\"]),\n",
    "            apply_to_links=tuple(ncfg[\"apply_to_links\"]),\n",
    "            clamp_min=float(ncfg[\"clamp_min\"]),\n",
    "        )\n",
    "    return ChannelBatchGenerator(bcfg, device=device, robust=robust, imperfect_cfg=icfg)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _sample_pair(gen: ChannelBatchGenerator):\n",
    "    out = gen.sample()\n",
    "    return out[\"g_imperfect\"], out[\"g_perfect\"]\n",
    "\n",
    "def epoch_iterator_standard(steps: int, batch_size: int, budgets: dict, robust: bool):\n",
    "    gen = _gen_base(batch_size=batch_size, robust=robust)\n",
    "    feat = FeatureConfig(include_system=False)\n",
    "    for _ in range(int(steps)):\n",
    "        g_imp, g_ref = _sample_pair(gen)\n",
    "        B = batch_size\n",
    "        tau = torch.full((B,), float(budgets[\"tau\"]), device=device)\n",
    "        ps_max = torch.full((B,), float(budgets[\"Ps_max\"]), device=device)\n",
    "        pr_max = torch.full((B,), float(budgets[\"Pr_max\"]), device=device)\n",
    "        x = build_features(g_imp, feat)\n",
    "        yield {\"x_imperfect\": x, \"g_perfect\": g_ref, \"tau\": tau, \"Ps_max\": ps_max, \"Pr_max\": pr_max}\n",
    "\n",
    "def epoch_iterator_genrob(steps: int, batch_size: int, robust: bool, tau_range, ps_range, pr_range, sigma: float = 0.0):\n",
    "    gen = _gen_base(batch_size=batch_size, robust=robust, sigma=sigma)\n",
    "    feat = FeatureConfig(include_system=True)\n",
    "    for _ in range(int(steps)):\n",
    "        g_imp, g_ref = _sample_pair(gen)\n",
    "        B = batch_size\n",
    "\n",
    "        # Sample *one* operating point per batch (stable + correct for DFParams/CFParams usage)\n",
    "        tau0 = float(np.random.uniform(tau_range[0], tau_range[1]))\n",
    "        ps0  = float(np.random.uniform(ps_range[0], ps_range[1]))\n",
    "        pr0  = float(np.random.uniform(pr_range[0], pr_range[1]))\n",
    "\n",
    "        tau = torch.full((B,), tau0, device=device)\n",
    "        ps_max = torch.full((B,), ps0, device=device)\n",
    "        pr_max = torch.full((B,), pr0, device=device)\n",
    "\n",
    "        x = build_features(g_imp, feat, tau=tau, ps_max=ps_max, pr_max=pr_max, si_db=None)\n",
    "        yield {\"x_imperfect\": x, \"g_perfect\": g_ref, \"tau\": tau, \"Ps_max\": ps_max, \"Pr_max\": pr_max}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e362671",
   "metadata": {},
   "source": [
    "## Train Base DF Policy\n",
    "\n",
    "$$\n",
    "\\text{DF step: } (P_S,P_R,\\alpha)=\\pi_{\\text{DF}}(x),\\quad x\\in\\mathbb{R}^8\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a33911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DF (base)\n",
    "\n",
    "df_budgets = {\"Ps_max\": float(df_cfg[\"power\"][\"Ps_max\"]), \"Pr_max\": float(df_cfg[\"power\"][\"Pr_max\"]), \"tau\": float(df_cfg[\"tau\"])}\n",
    "\n",
    "scfg_df = StepConfig(scheme=\"df\", safety=safety_on, rs_threshold=rs_th, outage_k=out_k)\n",
    "\n",
    "def df_step(model, batch, device):\n",
    "    return df_step_fn(model, batch, device, dfp=dfp_nom, w=w, risk=risk, cfg=scfg_df)\n",
    "\n",
    "df_model = DFPolicy(in_dim=8)\n",
    "\n",
    "base_df_paths = None\n",
    "base_df_metrics = None\n",
    "\n",
    "if RUN_BASE_TRAIN:\n",
    "    base_df_paths, base_df_metrics = train_policy(\n",
    "        stage=\"df_base\",\n",
    "        model=df_model,\n",
    "        step_fn=df_step,\n",
    "        epoch_iterator_fn=lambda steps: epoch_iterator_standard(steps, BATCH_BASE, df_budgets, robust=False),\n",
    "        epochs=EPOCHS_BASE,\n",
    "        steps_per_epoch=STEPS_BASE,\n",
    "        mixed_precision=bool(AMP),\n",
    "        opt_cfg=opt_cfg,\n",
    "        grad_clip_norm=grad_clip,\n",
    "    )\n",
    "    display(base_df_metrics.tail(10))\n",
    "else:\n",
    "    print(\"Skipping base DF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f13d59",
   "metadata": {},
   "source": [
    "## Train Base CF Policy\n",
    "\n",
    "$$\n",
    "\\text{CF step: } (P_S,P_R)=\\pi_{\\text{CF}}(x),\\quad x\\in\\mathbb{R}^8\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CF (base)\n",
    "\n",
    "cf_budgets = {\"Ps_max\": float(cf_cfg[\"power\"][\"Ps_max\"]), \"Pr_max\": float(cf_cfg[\"power\"][\"Pr_max\"]), \"tau\": float(cf_cfg[\"tau\"])}\n",
    "\n",
    "scfg_cf = StepConfig(scheme=\"cf\", safety=safety_on, rs_threshold=rs_th, outage_k=out_k)\n",
    "\n",
    "def cf_step(model, batch, device):\n",
    "    return cf_step_fn(model, batch, device, cfp=cfp_nom, w=w, risk=risk, cfg=scfg_cf)\n",
    "\n",
    "cf_model = CFPolicy(in_dim=8)\n",
    "\n",
    "base_cf_paths = None\n",
    "base_cf_metrics = None\n",
    "\n",
    "if RUN_BASE_TRAIN:\n",
    "    base_cf_paths, base_cf_metrics = train_policy(\n",
    "        stage=\"cf_base\",\n",
    "        model=cf_model,\n",
    "        step_fn=cf_step,\n",
    "        epoch_iterator_fn=lambda steps: epoch_iterator_standard(steps, BATCH_BASE, cf_budgets, robust=False),\n",
    "        epochs=EPOCHS_BASE,\n",
    "        steps_per_epoch=STEPS_BASE,\n",
    "        mixed_precision=bool(AMP),\n",
    "        opt_cfg=opt_cfg,\n",
    "        grad_clip_norm=grad_clip,\n",
    "    )\n",
    "    display(base_cf_metrics.tail(10))\n",
    "else:\n",
    "    print(\"Skipping base CF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de738964",
   "metadata": {},
   "source": [
    "## Train Generalized + Robust DF/CF Policies\n",
    "\n",
    "Input is \\(x\\in\\mathbb{R}^{11}\\) with appended \\((\\tau,P_{S,\\max},P_{R,\\max})\\).  \n",
    "Robust pairing uses \\(g_{\\text{imperfect}}\\) for input and \\(g_{\\text{perfect}}\\) for physics loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DF and CF (generalized + robust)\n",
    "\n",
    "gcfg = cfg.get(\"generalized\", {})\n",
    "tau_range = tuple(gcfg.get(\"tau_range\", [0.1, 0.9]))\n",
    "ps_range = tuple(gcfg.get(\"ps_max_range\", [1.0, 10.0]))\n",
    "pr_range = tuple(gcfg.get(\"pr_max_range\", [1.0, 10.0]))\n",
    "\n",
    "genrob_df_paths = None\n",
    "genrob_cf_paths = None\n",
    "genrob_df_metrics = None\n",
    "genrob_cf_metrics = None\n",
    "\n",
    "if RUN_GENROB_TRAIN:\n",
    "    # Generalized DF (11-dim)\n",
    "    gen_df_model = GeneralizedDFPolicy(in_dim=11)\n",
    "    def df_step_gen(model, batch, device):\n",
    "        # Use batchwise tau (constant per batch)\n",
    "        tau0 = float(batch[\"tau\"][0].item())\n",
    "        dfp = DFParams(tau=tau0, noise_power=dfp_nom.noise_power, Pp=dfp_nom.Pp, Rp0=dfp_nom.Rp0)\n",
    "        return df_step_fn(model, batch, device, dfp=dfp, w=w, risk=risk, cfg=scfg_df)\n",
    "\n",
    "    genrob_df_paths, genrob_df_metrics = train_policy(\n",
    "        stage=\"df_genrob\",\n",
    "        model=gen_df_model,\n",
    "        step_fn=df_step_gen,\n",
    "        epoch_iterator_fn=lambda steps: epoch_iterator_genrob(steps, BATCH_GENROB, robust=True, tau_range=tau_range, ps_range=ps_range, pr_range=pr_range),\n",
    "        epochs=EPOCHS_GENROB,\n",
    "        steps_per_epoch=STEPS_GENROB,\n",
    "        mixed_precision=bool(AMP),\n",
    "        opt_cfg=opt_cfg,\n",
    "        grad_clip_norm=grad_clip,\n",
    "    )\n",
    "    display(genrob_df_metrics.tail(10))\n",
    "\n",
    "    # Generalized CF (11-dim)\n",
    "    gen_cf_model = GeneralizedCFPolicy(in_dim=11)\n",
    "    def cf_step_gen(model, batch, device):\n",
    "        tau0 = float(batch[\"tau\"][0].item())\n",
    "        cfp = CFParams(tau=tau0, noise_power=cfp_nom.noise_power, Pp=cfp_nom.Pp, Rp0=cfp_nom.Rp0)\n",
    "        return cf_step_fn(model, batch, device, cfp=cfp, w=w, risk=risk, cfg=scfg_cf)\n",
    "\n",
    "    genrob_cf_paths, genrob_cf_metrics = train_policy(\n",
    "        stage=\"cf_genrob\",\n",
    "        model=gen_cf_model,\n",
    "        step_fn=cf_step_gen,\n",
    "        epoch_iterator_fn=lambda steps: epoch_iterator_genrob(steps, BATCH_GENROB, robust=True, tau_range=tau_range, ps_range=ps_range, pr_range=pr_range),\n",
    "        epochs=EPOCHS_GENROB,\n",
    "        steps_per_epoch=STEPS_GENROB,\n",
    "        mixed_precision=bool(AMP),\n",
    "        opt_cfg=opt_cfg,\n",
    "        grad_clip_norm=grad_clip,\n",
    "    )\n",
    "    display(genrob_cf_metrics.tail(10))\n",
    "else:\n",
    "    print(\"Skipping generalized+robust training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb08ebf",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "For each scheme we report:\n",
    "- Mean rate \\(\\mathbb{E}[R_S]\\)\n",
    "- QoS violation probability \\(\\mathbb{P}(Q>A)\\) and mean \\(\\mathbb{E}[(Q-A)^+]\\)\n",
    "- Outage probability \\(\\mathbb{P}(R_S<R_{S,\\text{th}})\\)\n",
    "- Mean energy efficiency \\(\\mathbb{E}[EE]\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c476705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utilities\n",
    "\n",
    "@torch.no_grad()\n",
    "def _policy_actions_df(model, x, ps_max, pr_max):\n",
    "    budgets = {\"Ps_max\": ps_max, \"Pr_max\": pr_max}\n",
    "    act = model.predict(x, budgets)\n",
    "    return act.ps, act.pr, act.alpha, act.gamma_ps, act.gamma_pr\n",
    "\n",
    "@torch.no_grad()\n",
    "def _policy_actions_cf(model, x, ps_max, pr_max):\n",
    "    budgets = {\"Ps_max\": ps_max, \"Pr_max\": pr_max}\n",
    "    act = model.predict(x, budgets)\n",
    "    return act.ps, act.pr, act.gamma_ps, act.gamma_pr\n",
    "\n",
    "@torch.no_grad()\n",
    "def _metrics_df(g, ps, pr, alpha, dfp, apply_safety: bool):\n",
    "    A = qos_A_df(g[\"PP\"], dfp)\n",
    "    if apply_safety:\n",
    "        ps, pr = safety_scale_joint_df(g, ps, pr, alpha, A)\n",
    "    rs = secondary_rate_df(g, ps, pr, alpha, dfp)\n",
    "    Q = qos_Q_df(g, ps, pr, alpha)\n",
    "    viol = qos_violation(Q, A)\n",
    "    out = outage_hard(rs, rs_th)\n",
    "    ee = ee_secondary(rs, ps, pr)\n",
    "    return rs, viol, out, ee\n",
    "\n",
    "@torch.no_grad()\n",
    "def _metrics_cf(g, ps, pr, cfp, apply_safety: bool):\n",
    "    A = qos_A_cf(g[\"PP\"], cfp)\n",
    "    if apply_safety:\n",
    "        ps, pr = safety_scale_joint_cf(g, ps, pr, A)\n",
    "    rs = secondary_rate_cf(g, ps, pr, cfp)\n",
    "    Q = qos_Q_cf(g, ps, pr)\n",
    "    viol = qos_violation(Q, A)\n",
    "    out = outage_hard(rs, rs_th)\n",
    "    ee = ee_secondary(rs, ps, pr)\n",
    "    return rs, viol, out, ee\n",
    "\n",
    "def _stack_links(g):\n",
    "    return torch.stack([g[k] for k in LINKS], dim=-1)\n",
    "\n",
    "def _snr_scale(g, snr_db: float):\n",
    "    scale = 10.0 ** (float(snr_db) / 10.0)\n",
    "    return {k: v * scale for k, v in g.items()}\n",
    "\n",
    "def _load_ckpt(model, ap: ArtifactPaths):\n",
    "    ckpt = ap.checkpoints / (\"best.pt\" if (ap.checkpoints / \"best.pt\").exists() else \"last.pt\")\n",
    "    sd = torch.load(ckpt, map_location=device)[\"model_state\"]\n",
    "    model.load_state_dict(sd)\n",
    "    model.to(device).eval()\n",
    "    return model, ckpt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cead0",
   "metadata": {},
   "source": [
    "## Evaluate Base Models vs Brute‑Force (SNR sweep)\n",
    "\n",
    "Hybrid schemes are compared under the same constraint handling (safety enabled).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base models (DF/CF/hybrid/rule) and brute-force baselines\n",
    "\n",
    "def evaluate_full(\n",
    "    *,\n",
    "    tag: str,\n",
    "    df_model,\n",
    "    cf_model,\n",
    "    generalized: bool,\n",
    "    apply_policy_safety: bool = True,\n",
    "    df_grid=DFGrid(**DF_GRID),\n",
    "    cf_grid=CFGrid(**CF_GRID),\n",
    "):\n",
    "    ap = make_artifacts(tag)\n",
    "\n",
    "    curves = {\n",
    "        \"DF(policy_safe)\": [],\n",
    "        \"CF(policy_safe)\": [],\n",
    "        \"Hybrid(rule)\": [],\n",
    "        \"DF(bruteforce)\": [],\n",
    "        \"CF(bruteforce)\": [],\n",
    "    }\n",
    "    curves_extra = {\n",
    "        \"EE_Hybrid\": [],\n",
    "        \"Outage_Hybrid\": [],\n",
    "        \"QoSviolProb_Hybrid\": [],\n",
    "    }\n",
    "\n",
    "    feat = FeatureConfig(include_system=bool(generalized))\n",
    "\n",
    "    pbar = tqdm(SNR_DB, desc=f\"eval:{tag}\", dynamic_ncols=True)\n",
    "    for snr in pbar:\n",
    "        # accumulators\n",
    "        rs_df, rs_cf, rs_hy = [], [], []\n",
    "        rs_dfb, rs_cfb = [], []\n",
    "        ee_hy, out_hy, qprob_hy = [], [], []\n",
    "\n",
    "        for _ in range(EVAL_STEPS):\n",
    "            g = generate_gains(int(EVAL_BATCH), ch[\"model\"], device, geo=geo, fad=fad, mixed_weights=ch.get(\"mixed\", {}).get(\"weights\"))\n",
    "            g = _snr_scale(g, snr)\n",
    "\n",
    "            B = int(EVAL_BATCH)\n",
    "            tau = torch.full((B,), float(df_cfg[\"tau\"]), device=device)\n",
    "            ps_max = torch.full((B,), float(df_cfg[\"power\"][\"Ps_max\"]), device=device)\n",
    "            pr_max = torch.full((B,), float(df_cfg[\"power\"][\"Pr_max\"]), device=device)\n",
    "\n",
    "            if generalized:\n",
    "                x = build_features(g, feat, tau=tau, ps_max=ps_max, pr_max=pr_max)\n",
    "            else:\n",
    "                x = build_features(g, feat)\n",
    "\n",
    "            # policy actions\n",
    "            ps_df_p, pr_df_p, a_df_p, *_ = _policy_actions_df(df_model, x, ps_max, pr_max)\n",
    "            ps_cf_p, pr_cf_p, *_ = _policy_actions_cf(cf_model, x, ps_max, pr_max)\n",
    "\n",
    "            # policy+safe metrics\n",
    "            rs_df_t, _, _, _ = _metrics_df(g, ps_df_p, pr_df_p, a_df_p, dfp_nom, apply_safety=apply_policy_safety)\n",
    "            rs_cf_t, _, _, _ = _metrics_cf(g, ps_cf_p, pr_cf_p, cfp_nom, apply_safety=apply_policy_safety)\n",
    "\n",
    "            # hybrid(rule)\n",
    "            hy = select_best_scheme(\n",
    "                g,\n",
    "                df_action={\"ps\": ps_df_p, \"pr\": pr_df_p, \"alpha\": a_df_p},\n",
    "                cf_action={\"ps\": ps_cf_p, \"pr\": pr_cf_p},\n",
    "                df_params=dfp_nom,\n",
    "                cf_params=cfp_nom,\n",
    "                safety=True,\n",
    "            )\n",
    "            rs_h = torch.where(hy[\"scheme\"] > 0.5, hy[\"rs_df\"], hy[\"rs_cf\"])\n",
    "            # compute hybrid metrics (use chosen actions already safety-scaled internally)\n",
    "            # We approximate metrics using rs_h and chosen ps/pr; alpha=0 for CF branch\n",
    "            ee = ee_secondary(rs_h, hy[\"ps\"], hy[\"pr\"])\n",
    "            out = outage_hard(rs_h, rs_th)\n",
    "            # QoS violation prob: compute both Q and A for the chosen scheme\n",
    "            # Conservative: compute DF violation for DF-chosen and CF violation for CF-chosen\n",
    "            A_df = qos_A_df(g[\"PP\"], dfp_nom); Q_df = qos_Q_df(g, hy[\"ps\"], hy[\"pr\"], hy[\"alpha\"]); viol_df = qos_violation(Q_df, A_df)\n",
    "            A_cf = qos_A_cf(g[\"PP\"], cfp_nom); Q_cf = qos_Q_cf(g, hy[\"ps\"], hy[\"pr\"]);               viol_cf = qos_violation(Q_cf, A_cf)\n",
    "            viol = torch.where(hy[\"scheme\"] > 0.5, viol_df, viol_cf)\n",
    "\n",
    "            # brute-force baselines\n",
    "            df_sol = solve_df_bruteforce(g, float(df_cfg[\"power\"][\"Ps_max\"]), float(df_cfg[\"power\"][\"Pr_max\"]), dfp_nom, df_grid)\n",
    "            cf_sol = solve_cf_bruteforce(g, float(cf_cfg[\"power\"][\"Ps_max\"]), float(cf_cfg[\"power\"][\"Pr_max\"]), cfp_nom, cf_grid)\n",
    "            rs_cf_b, _, _, _ = _metrics_cf(g, cf_sol[\"ps\"], cf_sol[\"pr\"], cfp_nom, apply_safety=False)\n",
    "\n",
    "            rs_df.append(float(rs_df_t.mean().item()))\n",
    "            rs_cf.append(float(rs_cf_t.mean().item()))\n",
    "            rs_hy.append(float(rs_h.mean().item()))\n",
    "            rs_dfb.append(float(df_sol[\"rs\"].mean().item()))\n",
    "            rs_cfb.append(float(rs_cf_b.mean().item()))\n",
    "\n",
    "            ee_hy.append(float(ee.mean().item()))\n",
    "            out_hy.append(float(out.mean().item()))\n",
    "            qprob_hy.append(float((viol > 0).float().mean().item()))\n",
    "\n",
    "        curves[\"DF(policy_safe)\"].append(float(np.mean(rs_df)))\n",
    "        curves[\"CF(policy_safe)\"].append(float(np.mean(rs_cf)))\n",
    "        curves[\"Hybrid(rule)\"].append(float(np.mean(rs_hy)))\n",
    "        curves[\"DF(bruteforce)\"].append(float(np.mean(rs_dfb)))\n",
    "        curves[\"CF(bruteforce)\"].append(float(np.mean(rs_cfb)))\n",
    "\n",
    "        curves_extra[\"EE_Hybrid\"].append(float(np.mean(ee_hy)))\n",
    "        curves_extra[\"Outage_Hybrid\"].append(float(np.mean(out_hy)))\n",
    "        curves_extra[\"QoSviolProb_Hybrid\"].append(float(np.mean(qprob_hy)))\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"SNR\": snr,\n",
    "            \"HY\": f\"{curves['Hybrid(rule)'][-1]:.3f}\",\n",
    "            \"CFb\": f\"{curves['CF(bruteforce)'][-1]:.3f}\",\n",
    "            \"Out\": f\"{curves_extra['Outage_Hybrid'][-1]:.2f}\",\n",
    "        })\n",
    "\n",
    "    payload = {\"snr_db\": SNR_DB, \"rs\": curves, \"extra\": curves_extra}\n",
    "    (ap.results / \"full_system_snr.json\").write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Plots\n",
    "    plt.figure()\n",
    "    for k, v in curves.items():\n",
    "        plt.plot(SNR_DB, v, label=k)\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"Average $R_S$\")\n",
    "    plt.title(f\"{tag}: RS vs SNR\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    fig1 = ap.figures / \"rs_vs_snr.png\"\n",
    "    plt.savefig(fig1, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(SNR_DB, curves_extra[\"EE_Hybrid\"], label=\"EE (Hybrid)\")\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"Average EE\")\n",
    "    plt.title(f\"{tag}: Energy efficiency (Hybrid)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    fig2 = ap.figures / \"ee_hybrid.png\"\n",
    "    plt.savefig(fig2, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(SNR_DB, curves_extra[\"Outage_Hybrid\"], label=\"Outage (Hybrid)\")\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"Outage probability\")\n",
    "    plt.title(f\"{tag}: Outage probability (Hybrid)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    fig3 = ap.figures / \"outage_hybrid.png\"\n",
    "    plt.savefig(fig3, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(SNR_DB, curves_extra[\"QoSviolProb_Hybrid\"], label=\"QoS violation prob (Hybrid)\")\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"P(Q>A)\")\n",
    "    plt.title(f\"{tag}: QoS violation probability (Hybrid)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    fig4 = ap.figures / \"qosviolprob_hybrid.png\"\n",
    "    plt.savefig(fig4, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Saved:\", ap.results / \"full_system_snr.json\")\n",
    "    print(\"Saved:\", fig1)\n",
    "    return ap, payload\n",
    "\n",
    "base_eval = None\n",
    "genrob_eval = None\n",
    "\n",
    "if RUN_EVALUATION:\n",
    "    if base_df_paths and base_cf_paths:\n",
    "        df_loaded, df_ckpt = _load_ckpt(DFPolicy(in_dim=8), base_df_paths)\n",
    "        cf_loaded, cf_ckpt = _load_ckpt(CFPolicy(in_dim=8), base_cf_paths)\n",
    "        print(\"Base ckpts:\", df_ckpt, cf_ckpt)\n",
    "        base_eval, base_payload = evaluate_full(tag=\"eval_base\", df_model=df_loaded, cf_model=cf_loaded, generalized=False)\n",
    "    if genrob_df_paths and genrob_cf_paths:\n",
    "        df_loaded, df_ckpt = _load_ckpt(GeneralizedDFPolicy(in_dim=11), genrob_df_paths)\n",
    "        cf_loaded, cf_ckpt = _load_ckpt(GeneralizedCFPolicy(in_dim=11), genrob_cf_paths)\n",
    "        print(\"GenRob ckpts:\", df_ckpt, cf_ckpt)\n",
    "        genrob_eval, genrob_payload = evaluate_full(tag=\"eval_genrob\", df_model=df_loaded, cf_model=cf_loaded, generalized=True)\n",
    "else:\n",
    "    print(\"Skipping evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421a3e8",
   "metadata": {},
   "source": [
    "## Robustness Evaluation (CSI noise sweep)\n",
    "\n",
    "We evaluate the same physics on \\(g_{\\text{perfect}}\\) while the policy observes \\(g_{\\text{imperfect}}\\) with varying \\(\\sigma\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0332fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness: compare base vs genrob under imperfect CSI at fixed SNR\n",
    "\n",
    "def eval_robust_sigma(tag: str, df_model, cf_model, generalized: bool, sigmas):\n",
    "    ap = make_artifacts(tag)\n",
    "\n",
    "    rows = []\n",
    "    feat = FeatureConfig(include_system=bool(generalized))\n",
    "\n",
    "    for sigma in sigmas:\n",
    "        rs_hy_list = []\n",
    "        out_list = []\n",
    "        qprob_list = []\n",
    "        ee_list = []\n",
    "\n",
    "        for _ in range(EVAL_STEPS):\n",
    "            g = generate_gains(int(EVAL_BATCH), ch[\"model\"], device, geo=geo, fad=fad, mixed_weights=ch.get(\"mixed\", {}).get(\"weights\"))\n",
    "            g = _snr_scale(g, GEN_EVAL_SNR_DB)\n",
    "\n",
    "            # Imperfect observation for input\n",
    "            icfg = ImperfectCSIConfig(sigma=float(sigma), apply_to_links=tuple(cfg[\"robust\"][\"noise\"][\"apply_to_links\"]), clamp_min=float(cfg[\"robust\"][\"noise\"][\"clamp_min\"]))\n",
    "            g_imp, g_ref = make_imperfect_pair(g, icfg)\n",
    "\n",
    "            B = int(EVAL_BATCH)\n",
    "            tau = torch.full((B,), float(df_cfg[\"tau\"]), device=device)\n",
    "            ps_max = torch.full((B,), float(df_cfg[\"power\"][\"Ps_max\"]), device=device)\n",
    "            pr_max = torch.full((B,), float(df_cfg[\"power\"][\"Pr_max\"]), device=device)\n",
    "\n",
    "            if generalized:\n",
    "                x = build_features(g_imp, feat, tau=tau, ps_max=ps_max, pr_max=pr_max)\n",
    "            else:\n",
    "                x = build_features(g_imp, feat)\n",
    "\n",
    "            ps_df_p, pr_df_p, a_df_p, *_ = _policy_actions_df(df_model, x, ps_max, pr_max)\n",
    "            ps_cf_p, pr_cf_p, *_ = _policy_actions_cf(cf_model, x, ps_max, pr_max)\n",
    "\n",
    "            hy = select_best_scheme(\n",
    "                g_ref,\n",
    "                df_action={\"ps\": ps_df_p, \"pr\": pr_df_p, \"alpha\": a_df_p},\n",
    "                cf_action={\"ps\": ps_cf_p, \"pr\": pr_cf_p},\n",
    "                df_params=dfp_nom,\n",
    "                cf_params=cfp_nom,\n",
    "                safety=True,\n",
    "            )\n",
    "            rs_h = torch.where(hy[\"scheme\"] > 0.5, hy[\"rs_df\"], hy[\"rs_cf\"])\n",
    "            ee = ee_secondary(rs_h, hy[\"ps\"], hy[\"pr\"])\n",
    "            out = outage_hard(rs_h, rs_th)\n",
    "\n",
    "            A_df = qos_A_df(g_ref[\"PP\"], dfp_nom); Q_df = qos_Q_df(g_ref, hy[\"ps\"], hy[\"pr\"], hy[\"alpha\"]); viol_df = qos_violation(Q_df, A_df)\n",
    "            A_cf = qos_A_cf(g_ref[\"PP\"], cfp_nom); Q_cf = qos_Q_cf(g_ref, hy[\"ps\"], hy[\"pr\"]);               viol_cf = qos_violation(Q_cf, A_cf)\n",
    "            viol = torch.where(hy[\"scheme\"] > 0.5, viol_df, viol_cf)\n",
    "\n",
    "            rs_hy_list.append(float(rs_h.mean().item()))\n",
    "            out_list.append(float(out.mean().item()))\n",
    "            qprob_list.append(float((viol > 0).float().mean().item()))\n",
    "            ee_list.append(float(ee.mean().item()))\n",
    "\n",
    "        rows.append({\n",
    "            \"sigma\": sigma,\n",
    "            \"rs_hybrid\": float(np.mean(rs_hy_list)),\n",
    "            \"outage\": float(np.mean(out_list)),\n",
    "            \"qos_viol_prob\": float(np.mean(qprob_list)),\n",
    "            \"ee\": float(np.mean(ee_list)),\n",
    "        })\n",
    "\n",
    "    df_res = pd.DataFrame(rows)\n",
    "    df_res.to_csv(ap.results / \"robust_sigma.csv\", index=False)\n",
    "    display(df_res)\n",
    "\n",
    "    # plots\n",
    "    plt.figure()\n",
    "    plt.plot(df_res[\"sigma\"], df_res[\"rs_hybrid\"])\n",
    "    plt.xlabel(\"CSI noise sigma\")\n",
    "    plt.ylabel(\"Hybrid mean $R_S$\")\n",
    "    plt.title(f\"{tag}: Robustness (RS vs sigma) @ SNR={GEN_EVAL_SNR_DB} dB\")\n",
    "    plt.grid(True)\n",
    "    fig1 = ap.figures / \"rs_vs_sigma.png\"\n",
    "    plt.savefig(fig1, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(df_res[\"sigma\"], df_res[\"outage\"])\n",
    "    plt.xlabel(\"CSI noise sigma\")\n",
    "    plt.ylabel(\"Outage probability\")\n",
    "    plt.title(f\"{tag}: Robustness (Outage vs sigma) @ SNR={GEN_EVAL_SNR_DB} dB\")\n",
    "    plt.grid(True)\n",
    "    fig2 = ap.figures / \"outage_vs_sigma.png\"\n",
    "    plt.savefig(fig2, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return ap, df_res\n",
    "\n",
    "if RUN_EVALUATION and base_df_paths and base_cf_paths:\n",
    "    df_loaded, _ = _load_ckpt(DFPolicy(in_dim=8), base_df_paths)\n",
    "    cf_loaded, _ = _load_ckpt(CFPolicy(in_dim=8), base_cf_paths)\n",
    "    _ = eval_robust_sigma(\"robust_base\", df_loaded, cf_loaded, generalized=False, sigmas=ROBUST_SIGMAS)\n",
    "\n",
    "if RUN_EVALUATION and genrob_df_paths and genrob_cf_paths:\n",
    "    df_loaded, _ = _load_ckpt(GeneralizedDFPolicy(in_dim=11), genrob_df_paths)\n",
    "    cf_loaded, _ = _load_ckpt(GeneralizedCFPolicy(in_dim=11), genrob_cf_paths)\n",
    "    _ = eval_robust_sigma(\"robust_genrob\", df_loaded, cf_loaded, generalized=True, sigmas=ROBUST_SIGMAS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335703f3",
   "metadata": {},
   "source": [
    "## Generalization Evaluation (system parameters sweep)\n",
    "\n",
    "We vary \\(\\tau, P_{S,\\max}, P_{R,\\max}\\) and report the resulting hybrid performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb30563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization: evaluate genrob model across (tau, Ps_max, Pr_max)\n",
    "\n",
    "def eval_generalization(tag: str, df_model, cf_model):\n",
    "    ap = make_artifacts(tag)\n",
    "    feat = FeatureConfig(include_system=True)\n",
    "\n",
    "    rows = []\n",
    "    for tau0 in TAU_GRID:\n",
    "        for ps0 in PS_MAX_GRID:\n",
    "            for pr0 in PR_MAX_GRID:\n",
    "                rs_hy_list = []\n",
    "                out_list = []\n",
    "                ee_list = []\n",
    "\n",
    "                for _ in range(EVAL_STEPS):\n",
    "                    g = generate_gains(int(EVAL_BATCH), ch[\"model\"], device, geo=geo, fad=fad, mixed_weights=ch.get(\"mixed\", {}).get(\"weights\"))\n",
    "                    g = _snr_scale(g, GEN_EVAL_SNR_DB)\n",
    "\n",
    "                    B = int(EVAL_BATCH)\n",
    "                    tau = torch.full((B,), float(tau0), device=device)\n",
    "                    ps_max = torch.full((B,), float(ps0), device=device)\n",
    "                    pr_max = torch.full((B,), float(pr0), device=device)\n",
    "\n",
    "                    x = build_features(g, feat, tau=tau, ps_max=ps_max, pr_max=pr_max)\n",
    "\n",
    "                    ps_df_p, pr_df_p, a_df_p, *_ = _policy_actions_df(df_model, x, ps_max, pr_max)\n",
    "                    ps_cf_p, pr_cf_p, *_ = _policy_actions_cf(cf_model, x, ps_max, pr_max)\n",
    "\n",
    "                    dfp = DFParams(tau=float(tau0), noise_power=dfp_nom.noise_power, Pp=dfp_nom.Pp, Rp0=dfp_nom.Rp0)\n",
    "                    cfp = CFParams(tau=float(tau0), noise_power=cfp_nom.noise_power, Pp=cfp_nom.Pp, Rp0=cfp_nom.Rp0)\n",
    "\n",
    "                    hy = select_best_scheme(\n",
    "                        g,\n",
    "                        df_action={\"ps\": ps_df_p, \"pr\": pr_df_p, \"alpha\": a_df_p},\n",
    "                        cf_action={\"ps\": ps_cf_p, \"pr\": pr_cf_p},\n",
    "                        df_params=dfp,\n",
    "                        cf_params=cfp,\n",
    "                        safety=True,\n",
    "                    )\n",
    "                    rs_h = torch.where(hy[\"scheme\"] > 0.5, hy[\"rs_df\"], hy[\"rs_cf\"])\n",
    "                    ee = ee_secondary(rs_h, hy[\"ps\"], hy[\"pr\"])\n",
    "                    out = outage_hard(rs_h, rs_th)\n",
    "\n",
    "                    rs_hy_list.append(float(rs_h.mean().item()))\n",
    "                    ee_list.append(float(ee.mean().item()))\n",
    "                    out_list.append(float(out.mean().item()))\n",
    "\n",
    "                rows.append({\n",
    "                    \"tau\": float(tau0),\n",
    "                    \"Ps_max\": float(ps0),\n",
    "                    \"Pr_max\": float(pr0),\n",
    "                    \"rs_hybrid\": float(np.mean(rs_hy_list)),\n",
    "                    \"ee\": float(np.mean(ee_list)),\n",
    "                    \"outage\": float(np.mean(out_list)),\n",
    "                })\n",
    "\n",
    "    df_res = pd.DataFrame(rows)\n",
    "    df_res.to_csv(ap.results / \"generalization_grid.csv\", index=False)\n",
    "    display(df_res.head())\n",
    "\n",
    "    # Plot: RS vs tau for each (Ps_max, Pr_max)\n",
    "    plt.figure()\n",
    "    for ps0 in PS_MAX_GRID:\n",
    "        for pr0 in PR_MAX_GRID:\n",
    "            sub = df_res[(df_res[\"Ps_max\"] == ps0) & (df_res[\"Pr_max\"] == pr0)].sort_values(\"tau\")\n",
    "            plt.plot(sub[\"tau\"], sub[\"rs_hybrid\"], label=f\"Ps={ps0},Pr={pr0}\")\n",
    "    plt.xlabel(r\"$\\tau$\")\n",
    "    plt.ylabel(\"Hybrid mean $R_S$\")\n",
    "    plt.title(f\"{tag}: Generalization (RS vs tau) @ SNR={GEN_EVAL_SNR_DB} dB\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    fig1 = ap.figures / \"rs_vs_tau.png\"\n",
    "    plt.savefig(fig1, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return ap, df_res\n",
    "\n",
    "if RUN_EVALUATION and genrob_df_paths and genrob_cf_paths:\n",
    "    df_loaded, _ = _load_ckpt(GeneralizedDFPolicy(in_dim=11), genrob_df_paths)\n",
    "    cf_loaded, _ = _load_ckpt(GeneralizedCFPolicy(in_dim=11), genrob_cf_paths)\n",
    "    _ = eval_generalization(\"generalization_genrob\", df_loaded, cf_loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03de93",
   "metadata": {},
   "source": [
    "## Selector Network (Supervised)\n",
    "\n",
    "We train a classifier \\(p_{\\theta}(\\text{DF}\\mid x)\\) with BCE loss against oracle labels:\n",
    "$$\n",
    "y=\\mathbb{1}\\{R_S^{\\text{DF,oracle}}\\ge R_S^{\\text{CF,oracle}}\\},\\quad\n",
    "\\mathcal{L}_{\\text{BCE}}=-y\\log p-(1-y)\\log(1-p).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfebcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train selector net + threshold tuning\n",
    "\n",
    "def build_selector_features(g, df_model, cf_model, generalized: bool, include_policy_outputs: bool):\n",
    "    B = g[\"PP\"].shape[0]\n",
    "    tau = torch.full((B,), float(df_cfg[\"tau\"]), device=device)\n",
    "    ps_max = torch.full((B,), float(df_cfg[\"power\"][\"Ps_max\"]), device=device)\n",
    "    pr_max = torch.full((B,), float(df_cfg[\"power\"][\"Pr_max\"]), device=device)\n",
    "\n",
    "    feat = FeatureConfig(include_system=bool(generalized))\n",
    "    if generalized:\n",
    "        x = build_features(g, feat, tau=tau, ps_max=ps_max, pr_max=pr_max)\n",
    "    else:\n",
    "        x = build_features(g, feat)\n",
    "\n",
    "    if not include_policy_outputs:\n",
    "        return x, tau, ps_max, pr_max\n",
    "\n",
    "    # Append policy outputs (gammas) to selector features\n",
    "    ps_df, pr_df, a_df, gps_df, gpr_df = _policy_actions_df(df_model, x, ps_max, pr_max)\n",
    "    ps_cf, pr_cf, gps_cf, gpr_cf = _policy_actions_cf(cf_model, x, ps_max, pr_max)\n",
    "\n",
    "    extras = torch.stack([gps_df, gpr_df, a_df, gps_cf, gpr_cf], dim=-1)  # [B,5]\n",
    "    x_sel = torch.cat([x, extras], dim=-1)\n",
    "    return x_sel, tau, ps_max, pr_max\n",
    "\n",
    "def train_selector(tag: str, df_model, cf_model, generalized: bool):\n",
    "    ap = make_artifacts(tag)\n",
    "\n",
    "    # Build one large dataset batch (oracle from brute force)\n",
    "    B = max(BATCH_SELECTOR, 2048)\n",
    "    g = generate_gains(int(B), ch[\"model\"], device, geo=geo, fad=fad, mixed_weights=ch.get(\"mixed\", {}).get(\"weights\"))\n",
    "    X, _, _, _ = build_selector_features(g, df_model, cf_model, generalized, SELECTOR_INCLUDE_POLICY_OUTPUTS)\n",
    "\n",
    "    # Oracle labels\n",
    "    df_sol = solve_df_bruteforce(g, float(df_cfg[\"power\"][\"Ps_max\"]), float(df_cfg[\"power\"][\"Pr_max\"]), dfp_nom, DFGrid(ps_steps=13, pr_steps=13, alpha_steps=7))\n",
    "    cf_sol = solve_cf_bruteforce(g, float(cf_cfg[\"power\"][\"Ps_max\"]), float(cf_cfg[\"power\"][\"Pr_max\"]), cfp_nom, CFGrid(ps_steps=21, pr_steps=21))\n",
    "    rs_cf = secondary_rate_cf(g, cf_sol[\"ps\"], cf_sol[\"pr\"], cfp_nom)\n",
    "    y = (df_sol[\"rs\"] >= rs_cf).float()\n",
    "\n",
    "    # DataLoader\n",
    "    ds = torch.utils.data.TensorDataset(X.detach().cpu(), y.detach().cpu())\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=min(512, BATCH_SELECTOR), shuffle=True, drop_last=False)\n",
    "\n",
    "    net = SelectorNet(in_dim=X.shape[1]).to(device)\n",
    "    opt = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    bce = torch.nn.BCELoss()\n",
    "\n",
    "    pbar = tqdm(range(1, EPOCHS_SELECTOR + 1), desc=f\"train:{tag}\", dynamic_ncols=True)\n",
    "    for ep in pbar:\n",
    "        net.train()\n",
    "        losses = []\n",
    "        accs = []\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            p = net(xb)\n",
    "            loss = bce(p, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.detach())\n",
    "            accs.append(((p >= 0.5).float() == yb).float().mean().detach())\n",
    "        loss_m = torch.stack(losses).mean().item()\n",
    "        acc_m = torch.stack(accs).mean().item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss_m:.4f}\", \"acc\": f\"{acc_m:.3f}\"})\n",
    "\n",
    "    # Threshold tuning on same batch\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        p_df = net(X)\n",
    "    t_grid = cfg[\"selector\"][\"threshold\"][\"grid\"]\n",
    "    best_t, info = tune_threshold(p_df, df_sol[\"rs\"], rs_cf, t_grid)\n",
    "    info[\"best_t\"] = best_t\n",
    "\n",
    "    torch.save({\"model_state\": net.state_dict(), \"best_t\": best_t, \"in_dim\": int(X.shape[1])}, ap.checkpoints / \"selector.pt\")\n",
    "    (ap.results / \"threshold_tuning.json\").write_text(json.dumps(info, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"best_t:\", best_t, \"best_mean_rs:\", info[\"best_rs\"])\n",
    "    return ap, net, best_t\n",
    "\n",
    "selector_ap = None\n",
    "selector_net = None\n",
    "selector_t = 0.5\n",
    "\n",
    "if RUN_SELECTOR_TRAIN and RUN_EVALUATION and base_df_paths and base_cf_paths:\n",
    "    df_loaded, _ = _load_ckpt(DFPolicy(in_dim=8), base_df_paths)\n",
    "    cf_loaded, _ = _load_ckpt(CFPolicy(in_dim=8), base_cf_paths)\n",
    "    selector_ap, selector_net, selector_t = train_selector(\"selector_base\", df_loaded, cf_loaded, generalized=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff3147",
   "metadata": {},
   "source": [
    "## Selector Evaluation (vs rule vs oracle)\n",
    "\n",
    "We compare:\n",
    "- Rule‑based hybrid\n",
    "- Learned selector + safety\n",
    "- Oracle (choose best of brute‑force DF/CF per sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate selector across SNR (base models)\n",
    "\n",
    "def eval_selector(tag: str, df_model, cf_model, net: SelectorNet, threshold: float, generalized: bool):\n",
    "    ap = make_artifacts(tag)\n",
    "    feat_g = FeatureConfig(include_system=bool(generalized))\n",
    "\n",
    "    curves = {\"RuleHybrid\": [], \"LearnedSelector\": [], \"Oracle\": []}\n",
    "\n",
    "    for snr in tqdm(SNR_DB, desc=f\"eval:{tag}\", dynamic_ncols=True):\n",
    "        rs_rule, rs_sel, rs_oracle = [], [], []\n",
    "\n",
    "        for _ in range(EVAL_STEPS):\n",
    "            g = generate_gains(int(EVAL_BATCH), ch[\"model\"], device, geo=geo, fad=fad, mixed_weights=ch.get(\"mixed\", {}).get(\"weights\"))\n",
    "            g = _snr_scale(g, snr)\n",
    "            B = int(EVAL_BATCH)\n",
    "\n",
    "            tau = torch.full((B,), float(df_cfg[\"tau\"]), device=device)\n",
    "            ps_max = torch.full((B,), float(df_cfg[\"power\"][\"Ps_max\"]), device=device)\n",
    "            pr_max = torch.full((B,), float(df_cfg[\"power\"][\"Pr_max\"]), device=device)\n",
    "\n",
    "            if generalized:\n",
    "                x = build_features(g, feat_g, tau=tau, ps_max=ps_max, pr_max=pr_max)\n",
    "            else:\n",
    "                x = build_features(g, feat_g)\n",
    "\n",
    "            ps_df, pr_df, a_df, gps_df, gpr_df = _policy_actions_df(df_model, x, ps_max, pr_max)\n",
    "            ps_cf, pr_cf, gps_cf, gpr_cf = _policy_actions_cf(cf_model, x, ps_max, pr_max)\n",
    "\n",
    "            # Rule hybrid\n",
    "            hy = select_best_scheme(\n",
    "                g,\n",
    "                df_action={\"ps\": ps_df, \"pr\": pr_df, \"alpha\": a_df},\n",
    "                cf_action={\"ps\": ps_cf, \"pr\": pr_cf},\n",
    "                df_params=dfp_nom,\n",
    "                cf_params=cfp_nom,\n",
    "                safety=True,\n",
    "            )\n",
    "            rs_h = torch.where(hy[\"scheme\"] > 0.5, hy[\"rs_df\"], hy[\"rs_cf\"])\n",
    "\n",
    "            # Learned selector feature\n",
    "            if SELECTOR_INCLUDE_POLICY_OUTPUTS:\n",
    "                extras = torch.stack([gps_df, gpr_df, a_df, gps_cf, gpr_cf], dim=-1)\n",
    "                x_sel = torch.cat([x, extras], dim=-1)\n",
    "            else:\n",
    "                x_sel = x\n",
    "\n",
    "            p_df = net(x_sel)\n",
    "            choose_df = p_df >= float(threshold)\n",
    "\n",
    "            # Apply safety + compute RS for chosen scheme\n",
    "            # DF metrics\n",
    "            rs_df_s, _, _, _ = _metrics_df(g, ps_df, pr_df, a_df, dfp_nom, apply_safety=True)\n",
    "            rs_cf_s, _, _, _ = _metrics_cf(g, ps_cf, pr_cf, cfp_nom, apply_safety=True)\n",
    "            rs_l = torch.where(choose_df, rs_df_s, rs_cf_s)\n",
    "\n",
    "            # Oracle from brute force\n",
    "            df_sol = solve_df_bruteforce(g, float(df_cfg[\"power\"][\"Ps_max\"]), float(df_cfg[\"power\"][\"Pr_max\"]), dfp_nom, DFGrid(ps_steps=13, pr_steps=13, alpha_steps=7))\n",
    "            cf_sol = solve_cf_bruteforce(g, float(cf_cfg[\"power\"][\"Ps_max\"]), float(cf_cfg[\"power\"][\"Pr_max\"]), cfp_nom, CFGrid(ps_steps=21, pr_steps=21))\n",
    "            rs_cf_or, _, _, _ = _metrics_cf(g, cf_sol[\"ps\"], cf_sol[\"pr\"], cfp_nom, apply_safety=False)\n",
    "            rs_or = torch.where(df_sol[\"rs\"] >= rs_cf_or, df_sol[\"rs\"], rs_cf_or)\n",
    "\n",
    "            rs_rule.append(float(rs_h.mean().item()))\n",
    "            rs_sel.append(float(rs_l.mean().item()))\n",
    "            rs_oracle.append(float(rs_or.mean().item()))\n",
    "\n",
    "        curves[\"RuleHybrid\"].append(float(np.mean(rs_rule)))\n",
    "        curves[\"LearnedSelector\"].append(float(np.mean(rs_sel)))\n",
    "        curves[\"Oracle\"].append(float(np.mean(rs_oracle)))\n",
    "\n",
    "    (ap.results / \"selector_snr.json\").write_text(json.dumps({\"snr_db\": SNR_DB, \"rs\": curves, \"threshold\": float(threshold)}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    plt.figure()\n",
    "    for k, v in curves.items():\n",
    "        plt.plot(SNR_DB, v, label=k)\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"Average $R_S$\")\n",
    "    plt.title(f\"{tag}: selector vs rule vs oracle\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    fig = ap.figures / \"selector_vs_rule.png\"\n",
    "    plt.savefig(fig, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved:\", fig)\n",
    "\n",
    "    return ap, curves\n",
    "\n",
    "if RUN_EVALUATION and selector_net is not None and base_df_paths and base_cf_paths:\n",
    "    df_loaded, _ = _load_ckpt(DFPolicy(in_dim=8), base_df_paths)\n",
    "    cf_loaded, _ = _load_ckpt(CFPolicy(in_dim=8), base_cf_paths)\n",
    "    _ = eval_selector(\"selector_eval_base\", df_loaded, cf_loaded, selector_net, selector_t, generalized=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea63a35",
   "metadata": {},
   "source": [
    "## Deployment Indicators\n",
    "\n",
    "We report:\n",
    "- parameter count and model size\n",
    "- latency vs batch size on the active device\n",
    "- TorchScript and ONNX export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651404eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment indicators (params/size/latency/export)\n",
    "\n",
    "def deploy_report(tag: str, model: torch.nn.Module, in_dim: int):\n",
    "    ap = make_artifacts(tag)\n",
    "\n",
    "    info = count_parameters(model)\n",
    "    size = model_size_mb(model)\n",
    "\n",
    "    lat = benchmark_model(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        in_dim=int(in_dim),\n",
    "        batch_sizes=[1, 8, 32, 128, 256],\n",
    "        cfg=BenchmarkConfig(warmup_iters=30, measure_iters=100),\n",
    "    )\n",
    "    df_lat = pd.DataFrame([{\"batch\": k, \"latency_ms\": v} for k, v in lat.items()]).sort_values(\"batch\")\n",
    "\n",
    "    (ap.results / \"deploy_report.json\").write_text(json.dumps({\"params\": info, \"size_mb\": size, \"latency_ms\": lat}, indent=2), encoding=\"utf-8\")\n",
    "    df_lat.to_csv(ap.results / \"latency.csv\", index=False)\n",
    "\n",
    "    display(pd.DataFrame([{\"total_params\": info[\"total_params\"], \"trainable_params\": info[\"trainable_params\"], \"size_mb\": size}]))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(df_lat[\"batch\"], df_lat[\"latency_ms\"])\n",
    "    plt.xlabel(\"Batch size\")\n",
    "    plt.ylabel(\"Latency (ms)\")\n",
    "    plt.title(f\"{tag}: latency benchmark ({device.type})\")\n",
    "    plt.grid(True)\n",
    "    fig = ap.figures / \"latency.png\"\n",
    "    plt.savefig(fig, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Export\n",
    "    try:\n",
    "        ts_path = export_torchscript(model.to(device).eval(), ap.results / \"model.ts\", in_dim=int(in_dim), device=device)\n",
    "        print(\"TorchScript:\", ts_path)\n",
    "    except Exception as e:\n",
    "        print(\"TorchScript export failed:\", e)\n",
    "\n",
    "    try:\n",
    "        onnx_path = export_onnx(model.to(device).eval(), ap.results / \"model.onnx\", in_dim=int(in_dim), device=device)\n",
    "        print(\"ONNX:\", onnx_path)\n",
    "    except Exception as e:\n",
    "        print(\"ONNX export failed:\", e)\n",
    "\n",
    "    return ap\n",
    "\n",
    "if RUN_DEPLOYMENT:\n",
    "    if base_df_paths:\n",
    "        m, _ = _load_ckpt(DFPolicy(in_dim=8), base_df_paths)\n",
    "        _ = deploy_report(\"deploy_df_base\", m, in_dim=8)\n",
    "    if base_cf_paths:\n",
    "        m, _ = _load_ckpt(CFPolicy(in_dim=8), base_cf_paths)\n",
    "        _ = deploy_report(\"deploy_cf_base\", m, in_dim=8)\n",
    "    if genrob_df_paths:\n",
    "        m, _ = _load_ckpt(GeneralizedDFPolicy(in_dim=11), genrob_df_paths)\n",
    "        _ = deploy_report(\"deploy_df_genrob\", m, in_dim=11)\n",
    "    if genrob_cf_paths:\n",
    "        m, _ = _load_ckpt(GeneralizedCFPolicy(in_dim=11), genrob_cf_paths)\n",
    "        _ = deploy_report(\"deploy_cf_genrob\", m, in_dim=11)\n",
    "else:\n",
    "    print(\"Skipping deployment.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
